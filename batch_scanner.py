# xss_security_gui/batch_scanner.py
from crawler import crawl_site, save_outputs
from datetime import datetime
import os
import json
import csv
import time
import re
from typing import List, Dict, Any, Tuple, Union

from multiprocessing import Pool, TimeoutError, cpu_count

# === –£–Ω–∏–≤–µ—Ä—Å–∞–ª—å–Ω—ã–µ –ø—É—Ç–∏ –≤–Ω—É—Ç—Ä–∏ –ø–∞–∫–µ—Ç–∞ ===
BASE_DIR = os.path.dirname(__file__)
LOGS_DIR = os.path.join(BASE_DIR, "logs")
OUTPUT_DIR = os.path.join(LOGS_DIR, "batch_sites")
REPORT_CSV = os.path.join(LOGS_DIR, "scan_report.csv")
FINAL_JSON = os.path.join(LOGS_DIR, "final_result.json")

# === –ü–∞—Ä–∞–º–µ—Ç—Ä—ã —Ñ–∏–ª—å—Ç—Ä–∞—Ü–∏–∏ –∏ –ø–∞—É–∑—ã ===
FILTER_ONLY_GOV = True
FILTER_EXCLUDE_WWW = True
SLEEP_BETWEEN = 0.5  # –º–µ–∂–¥—É —Å—Ç–∞—Ä—Ç–∞–º–∏ –ø—Ä–æ—Ü–µ—Å—Å–æ–≤

# === –ü–∞—Ä–∞–º–µ—Ç—Ä—ã –ø–∞—Ä–∞–ª–ª–µ–ª–∏–∑–º–∞ –∏ —Ç–∞–π–º–∞—É—Ç–æ–≤ ===
MAX_PROCESSES = max(2, cpu_count() // 2)
PER_TARGET_TIMEOUT = 120  # —Å–µ–∫—É–Ω–¥ –Ω–∞ –æ–¥–∏–Ω —Å–∞–π—Ç


def ensure_dirs() -> None:
    os.makedirs(LOGS_DIR, exist_ok=True)
    os.makedirs(OUTPUT_DIR, exist_ok=True)


def load_targets(path: str = os.path.join(BASE_DIR, "targets.txt")) -> List[str]:
    """
    –ó–∞–≥—Ä—É–∂–∞–µ—Ç —Ü–µ–ª–∏ –∏–∑ —Ñ–∞–π–ª–∞, –ø—Ä–∏–º–µ–Ω—è–µ—Ç —Ñ–∏–ª—å—Ç—Ä—ã:
    - —Ç–æ–ª—å–∫–æ .gov –¥–æ–º–µ–Ω—ã (–µ—Å–ª–∏ FILTER_ONLY_GOV=True)
    - –∏—Å–∫–ª—é—á–∞–µ—Ç www (–µ—Å–ª–∏ FILTER_EXCLUDE_WWW=True)
    """
    if not os.path.exists(path):
        raise FileNotFoundError(f"targets.txt –Ω–µ –Ω–∞–π–¥–µ–Ω: {path}")

    with open(path, encoding="utf-8") as f:
        raw = [line.strip() for line in f if line.strip().startswith("http")]
        targets: List[str] = []

        for url in raw:
            if FILTER_ONLY_GOV and not re.search(r"\.gov(\.|/|$)", url):
                continue
            if FILTER_EXCLUDE_WWW and "//www." in url:
                continue
            targets.append(url)

        return targets


def safe_domain_filename(url: str) -> str:
    """
    –ü—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç URL –≤ –±–µ–∑–æ–ø–∞—Å–Ω–æ–µ –∏–º—è —Ñ–∞–π–ª–∞.
    """
    domain = url.replace("http://", "").replace("https://", "")
    domain = domain.replace("/", "_")
    return f"{domain}.json"


def write_json_result(url: str, data: Dict[str, Any]) -> str:
    """
    –°–æ—Ö—Ä–∞–Ω—è–µ—Ç per‚Äëtarget JSON –≤ OUTPUT_DIR.
    –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç –ø—É—Ç—å –∫ —Ñ–∞–π–ª—É.
    """
    ensure_dirs()
    filename = os.path.join(OUTPUT_DIR, safe_domain_filename(url))
    with open(filename, "w", encoding="utf-8") as f:
        json.dump(data, f, indent=2, ensure_ascii=False)
    return filename


def append_csv_report(rows: List[List[Any]]) -> None:
    """
    –î–æ–±–∞–≤–ª—è–µ—Ç —Å—Ç—Ä–æ–∫–∏ –≤ CSV‚Äë–æ—Ç—á—ë—Ç. –ó–∞–≥–æ–ª–æ–≤–æ–∫ –ø–∏—à–µ—Ç—Å—è –æ–¥–∏–Ω —Ä–∞–∑.
    """
    ensure_dirs()
    header = [
        "URL", "Forms", "Scripts", "Links", "CMS", "Frameworks",
        "Tokens", "GraphQL", "SourceMaps", "Adaptive", "Score",
        "Status", "DurationSec"
    ]
    file_exists = os.path.exists(REPORT_CSV)

    with open(REPORT_CSV, "a", newline="", encoding="utf-8") as f:
        writer = csv.writer(f)
        if not file_exists:
            writer.writerow(header)
        for row in rows:
            writer.writerow(row)


def normalize_node_result(result: Union[Dict[str, Any], Tuple[List[Dict[str, Any]], List[str]]]) -> Dict[str, Any]:
    """
    –£–Ω–∏—Ñ–∏—Ü–∏—Ä—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç crawl_site:
    - –µ—Å–ª–∏ crawl_site –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç –æ–¥–∏–Ω —É–∑–µ–ª (dict) ‚Üí –≤–æ–∑–≤—Ä–∞—â–∞–µ–º –∫–∞–∫ –µ—Å—Ç—å
    - –µ—Å–ª–∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç (raw, pages) ‚Üí —Å–æ–±–∏—Ä–∞–µ–º —Å–≤–æ–¥–Ω—ã–π —É–∑–µ–ª
    """
    if isinstance(result, dict):
        return result

    if isinstance(result, tuple) and len(result) == 2:
        raw, pages = result

        forms = sum(len(n.get("forms", [])) for n in raw)
        scripts = sum(len(n.get("scripts", [])) for n in raw)
        links = sum(len(n.get("links", [])) for n in raw)
        adaptive = any(n.get("adaptive") for n in raw)
        tokens_list: List[str] = []
        graphql_count = 0
        sourcemaps_count = 0
        content_score = 0
        frameworks: List[str] = []
        ipv4_list: List[str] = []
        ipv6_list: List[str] = []

        for n in raw:
            tokens_list.extend(n.get("tokens", []))
            graphql_count += len(n.get("graphql", []))
            sourcemaps_count += len(n.get("sourcemaps", []))
            content_score += n.get("content_score", 0)
            frameworks.extend(n.get("frameworks", []) or [])
            ipv4_list.extend(n.get("ipv4", []))
            ipv6_list.extend(n.get("ipv6", []))

        # –¥–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è —Å–ø–∏—Å–∫–æ–≤
        def _dedupe(seq: List[Any]) -> List[Any]:
            return list(dict.fromkeys(seq))

        node = {
            "forms": forms,
            "scripts": scripts,
            "links": links,
            "pages": pages,
            "adaptive": adaptive,
            "tokens": _dedupe(tokens_list),
            "graphql": graphql_count,
            "sourcemaps": sourcemaps_count,
            "content_score": content_score,
            "cms": "-",
            "frameworks": _dedupe(frameworks),
            "ipv4": _dedupe(ipv4_list),
            "ipv6": _dedupe(ipv6_list),
        }
        return node

    return {"error": "unexpected_result_format"}


# === –û–±—ë—Ä—Ç–∫–∞ –¥–ª—è multiprocessing ===

def _scan_single_target(url: str) -> Dict[str, Any]:
    """
    –§—É–Ω–∫—Ü–∏—è-–æ–±—ë—Ä—Ç–∫–∞ –¥–ª—è –ø—Ä–æ—Ü–µ—Å—Å–∞:
    - –≤—ã–∑—ã–≤–∞–µ—Ç crawl_site(url)
    - –Ω–æ—Ä–º–∞–ª–∏–∑—É–µ—Ç —Ä–µ–∑—É–ª—å—Ç–∞—Ç
    - –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç dict —Å –ø–æ–ª—è–º–∏:
      { "url": ..., "node": ..., "error": None | str, "duration": float }
    """
    start_ts = time.time()
    try:
        result = crawl_site(url)
        node = normalize_node_result(result)
        duration = round(time.time() - start_ts, 2)
        return {
            "url": url,
            "node": node,
            "error": None,
            "duration": duration,
        }
    except Exception as e:
        duration = round(time.time() - start_ts, 2)
        return {
            "url": url,
            "node": {},
            "error": f"{type(e).__name__}: {e}",
            "duration": duration,
        }


def run_batch(file_path: str = os.path.join(BASE_DIR, "targets.txt")) -> None:
    """
    –ü–∞–∫–µ—Ç–Ω—ã–π –æ–±—Ö–æ–¥ —Ü–µ–ª–µ–π (–≥–∏–±—Ä–∏–¥: multiprocessing + threads –≤–Ω—É—Ç—Ä–∏ –∫—Ä–∞—É–ª–µ—Ä–∞):
    - –∫–∞–∂–¥—ã–π —Å–∞–π—Ç —Å–∫–∞–Ω–∏—Ä—É–µ—Ç—Å—è –≤ –æ—Ç–¥–µ–ª—å–Ω–æ–º –ø—Ä–æ—Ü–µ—Å—Å–µ
    - –≤–Ω—É—Ç—Ä–∏ crawl_site —É–∂–µ –º–æ–∂–µ—Ç –±—ã—Ç—å –ø–∞—Ä–∞–ª–ª–µ–ª—å–Ω—ã–π –æ–±—Ö–æ–¥ —Å—Å—ã–ª–æ–∫
    - —Å–æ—Ö—Ä–∞–Ω—è–µ—Ç per‚Äëtarget JSON
    - –¥–æ–ø–æ–ª–Ω—è–µ—Ç CSV‚Äë–æ—Ç—á—ë—Ç
    - —Ñ–æ—Ä–º–∏—Ä—É–µ—Ç —Ñ–∏–Ω–∞–ª—å–Ω—ã–π —Å–≤–æ–¥–Ω—ã–π JSON —Å –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã–º–∏ IPv4/IPv6/tokens
    """
    ensure_dirs()
    targets = load_targets(file_path)
    all_csv_rows: List[List[Any]] = []
    final_items: List[Dict[str, Any]] = []

    print(f"üîé –í—Å–µ–≥–æ –æ—Ç—Ñ–∏–ª—å—Ç—Ä–æ–≤–∞–Ω–æ —Ü–µ–ª–µ–π: {len(targets)}")
    success_count = 0
    error_count = 0

    if not targets:
        print("‚ö†Ô∏è –ù–µ—Ç —Ü–µ–ª–µ–π –¥–ª—è —Å–∫–∞–Ω–∏—Ä–æ–≤–∞–Ω–∏—è.")
        return

    aggregated_tokens: List[str] = []
    aggregated_ipv4: List[str] = []
    aggregated_ipv6: List[str] = []

    with Pool(processes=MAX_PROCESSES) as pool:
        async_results = []
        for idx, url in enumerate(targets, start=1):
            print(f"[{idx}/{len(targets)}] üåê –ü–ª–∞–Ω–∏—Ä—É–µ–º –æ–±—Ö–æ–¥: {url}")
            async_res = pool.apply_async(_scan_single_target, (url,))
            async_results.append((url, async_res))
            time.sleep(SLEEP_BETWEEN)

        for idx, (url, async_res) in enumerate(async_results, start=1):
            print(f"\n[{idx}/{len(targets)}] üöÄ –û–±—Ä–∞–±–æ—Ç–∫–∞ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞: {url}")
            start_ts = time.time()
            try:
                res: Dict[str, Any] = async_res.get(timeout=PER_TARGET_TIMEOUT)
            except TimeoutError:
                err_msg = f"TimeoutError: –ø—Ä–µ–≤—ã—à–µ–Ω –ª–∏–º–∏—Ç {PER_TARGET_TIMEOUT}s"
                print(f"‚ùå –û—à–∏–±–∫–∞: {err_msg}")
                error_count += 1

                all_csv_rows.append([
                    url, 0, 0, 0, "-", "-", 0, 0, 0, "‚úò", 0,
                    "TIMEOUT", round(time.time() - start_ts, 2)
                ])

                try:
                    with open(os.path.join(LOGS_DIR, "batch_errors.log"), "a", encoding="utf-8") as errlog:
                        errlog.write(f"[{datetime.now().isoformat()}] {url} ‚Üí {err_msg}\n")
                except Exception:
                    pass

                continue
            except Exception as e:
                err_msg = f"{type(e).__name__}: {e}"
                print(f"‚ùå –û—à–∏–±–∫–∞: {err_msg}")
                error_count += 1

                all_csv_rows.append([
                    url, 0, 0, 0, "-", "-", 0, 0, 0, "‚úò", 0,
                    "ERROR", round(time.time() - start_ts, 2)
                ])

                try:
                    with open(os.path.join(LOGS_DIR, "batch_errors.log"), "a", encoding="utf-8") as errlog:
                        errlog.write(f"[{datetime.now().isoformat()}] {url} ‚Üí {err_msg}\n")
                except Exception:
                    pass

                continue

            node = res.get("node") or {}
            error = res.get("error")
            duration = res.get("duration", round(time.time() - start_ts, 2))

            if error:
                print(f"‚ùå –û—à–∏–±–∫–∞: {error}")
                error_count += 1

                all_csv_rows.append([
                    url, 0, 0, 0, "-", "-", 0, 0, 0, "‚úò", 0,
                    "ERROR", duration
                ])

                try:
                    with open(os.path.join(LOGS_DIR, "batch_errors.log"), "a", encoding="utf-8") as errlog:
                        errlog.write(f"[{datetime.now().isoformat()}] {url} ‚Üí {error}\n")
                except Exception:
                    pass

                continue

            # –ê–≥—Ä–µ–≥–∞—Ü–∏—è –¥–ª—è Threat Intel / –∞–≤—Ç–æ–∞—Ç–∞–∫
            aggregated_tokens.extend(node.get("tokens", []))
            aggregated_ipv4.extend(node.get("ipv4", []))
            aggregated_ipv6.extend(node.get("ipv6", []))

            out_path = write_json_result(url, node)

            csv_row = [
                url,
                len(node.get("forms", [])),
                len(node.get("scripts", [])),
                len(node.get("links", [])),
                node.get("cms", "-"),
                ", ".join(node.get("frameworks", [])) if node.get("frameworks") else "-",
                len(node.get("tokens", [])),
                len(node.get("graphql", [])) if isinstance(node.get("graphql"), list) else node.get("graphql", 0),
                len(node.get("sourcemaps", [])) if isinstance(node.get("sourcemaps"), list) else node.get("sourcemaps", 0),
                "‚úÖ" if node.get("adaptive") else "‚úò",
                node.get("content_score", 0),
                "OK",
                duration,
            ]
            all_csv_rows.append(csv_row)
            final_items.append({"url": url, "result_path": out_path, "node": node})
            success_count += 1

    # –î–µ–¥—É–ø–ª–∏–∫–∞—Ü–∏—è –∞–≥—Ä–µ–≥–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö
    def _dedupe(seq: List[str]) -> List[str]:
        return list(dict.fromkeys(seq))

    aggregated_tokens = _dedupe(aggregated_tokens)
    aggregated_ipv4 = _dedupe(aggregated_ipv4)
    aggregated_ipv6 = _dedupe(aggregated_ipv6)

    if all_csv_rows:
        append_csv_report(all_csv_rows)
        print(f"\nüìä CSV-–æ—Ç—á—ë—Ç –¥–æ–±–∞–≤–ª–µ–Ω –≤ {REPORT_CSV}")

    final_result = {
        "timestamp": datetime.now().isoformat(),
        "targets_count": len(targets),
        "success": success_count,
        "errors": error_count,
        "items": final_items,
        "filters": {
            "FILTER_ONLY_GOV": FILTER_ONLY_GOV,
            "FILTER_EXCLUDE_WWW": FILTER_EXCLUDE_WWW,
        },
        "sleep_between_sec": SLEEP_BETWEEN,
        "max_processes": MAX_PROCESSES,
        "per_target_timeout_sec": PER_TARGET_TIMEOUT,
        "aggregated": {
            "tokens": aggregated_tokens,
            "ipv4": aggregated_ipv4,
            "ipv6": aggregated_ipv6,
        },
    }

    with open(FINAL_JSON, "w", encoding="utf-8") as f:
        json.dump(final_result, f, indent=2, ensure_ascii=False)

    try:
        save_outputs(final_result)
    except TypeError as e:
        print(f"‚ö†Ô∏è save_outputs –≤—ã–∑–≤–∞–Ω —Å final_result, –Ω–æ –≤–æ–∑–Ω–∏–∫–ª–∞ –æ—à–∏–±–∫–∞: {e}")
        print(f"‚ÑπÔ∏è –§–∏–Ω–∞–ª—å–Ω—ã–π JSON —Å–æ—Ö—Ä–∞–Ω—ë–Ω: {FINAL_JSON}")

    print("‚úÖ –í—Å–µ —Å–∫–∞–Ω—ã –∑–∞–≤–µ—Ä—à–µ–Ω—ã.")
    print(f"üìÅ –ê—Ä—Ç–µ—Ñ–∞–∫—Ç—ã: {OUTPUT_DIR}")
    print(f"üìÑ –°–≤–æ–¥–∫–∞: {FINAL_JSON}")
    print(f"üßæ –û—Ç—á—ë—Ç: {REPORT_CSV}")


if __name__ == "__main__":
    run_batch()